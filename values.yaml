# Defaults to bare-bones boundary installation without auth method, host resources, scopes, targets, or initial login role creation
# requires recovery key workflow to run commands
# see here for more info: https://developer.hashicorp.com/boundary/docs/install-boundary/no-gen-resources
# if quick POC is desired instead, set dev to "true" below
boundary: 
  replicas: 1
    # TO ENABLE ENTERPRISE
    # set licenseSecretName to k8s secret for the boundary enterprise license 
    # template will auto update repo to enterprise, and auto append -ent to tag, regardless if tag already has -ent suffix
    # example secret creation:
    # kubectl create secret generic boundary-license --from-literal=license=${BOUNDARY_LICENSE}
    # with 'boundary-license' being the value for licenseSecretName etc
  image:
    licenseSecretName: "boundary-license"
    licenseSecretKey: license
    repository: hashicorp/boundary
    tag: latest
    pullPolicy: IfNotPresent
  
  setup:
    # dev mode creates root user "admin" with password "password"; run "boundary authenticate" to login
    # defaults to true for quick poc; set to false to allow for below 
    dev: false
    
    # setting to false as will require recovery key (kms) workflow and bootsrap w/o any resources
    # kms workflow outlined here https://developer.hashicorp.com/boundary/docs/install-boundary/no-gen-resources 
    # set to true to start in non-dev with all init options from here: 
    # https://developer.hashicorp.com/boundary/docs/commands/database/init#init-options 
    # must tail logs of init container to get credentials: 
    # kubectl logs boundary-controller-0 boundary-init -f
    withResources: true

    # migrateDB is used only after initialization if you wish to update the config
    # postgres pvc must have been created upon initialization (line 163)
    # if updating "root" kms key in config, KMS migration is also required: 
    # - keep the old "root" kms stanza in config along with your desired new "root" kms stanza
    # - change the 'purpose' parameter of this old kms stanza from "root" to "previous-root"
    migrateDB: false
  
  config: |
    disable_mlock = true

    controller {
      name = "kubernetes-controller"
      description = "A controller for a kubernetes demo!"
      database {
        url = "env://BOUNDARY_PG_URL"
      }
      public_cluster_addr = "boundary-controller.default.svc.cluster.local:9201"
    }


    listener "tcp" {
      address = "0.0.0.0:9200"
      purpose = "api"
      tls_disable = true
    }

    listener "tcp" {
      address = "0.0.0.0:9201"
      purpose = "cluster"
      tls_disable = true
    }

    listener "tcp" {
      address = "0.0.0.0:9203"
      purpose = "proxy"
      tls_disable = true
    }

    kms "aead" {
      purpose = "root"
      aead_type = "aes-gcm"
      key = "sP1fnF5Xz85RrXyELHFeZg9Ad2qt4Z4bgNHVGtD6ung="
      key_id = "global_root"
    }
    
    kms "aead" {
      purpose = "worker-auth"
      aead_type = "aes-gcm"
      key = "8fZBjCUfN0TzjEGLQldGY4+iE9AkOvCfjh7+p0GtRBQ="
      key_id = "global_worker-auth"
    }

    kms "aead" {
      purpose = "recovery"
      aead_type = "aes-gcm"
      key = "8fZBjCUfN0TzjEGLQldGY4+iE9AkOvCfjh7+p0GtRBQ="
      key_id = "global_recovery"
    }
    kms "aead" {
      purpose = "bsr"
      aead_type = "aes-gcm"
      key = "8fZBjCUfN0TzjEGLQldGY4+iE9AkOvCfjh7+p0GtRBQ="
      key_id = "global_bsr"
    }

  readinessProbe:
    exec:
      command:
        - sh
        - -c
        - "nc -zv postgres-boundary 5432"
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3

  livenessProbe: 
    httpGet:
      path: "/"
      port: 9200



  serviceAccount:
    # Specifies whether a service account should be created
    create: false
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  podAnnotations: {}

  service:
    type: ClusterIP
    name: boundary-controller
    ports:
    - name: api
      port: 9200
    - name: cluster
      port: 9201
    - name: data
      port: 9203

  ingress:
    enabled: false
    className: ""
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  resources:
    # for minikube mac m1
    requests:
      cpu: "100m"  # 0.1 CPU cores
      memory: "256Mi"  # 256 MiB of memory
    limits:
      cpu: "500m"  # 0.5 CPU cores
      memory: "1Gi"  # 1 GiB of memory  
  
  worker:
    ingress:
      enabled: false
      replicas: 1
      config: |
        # disable memory from being swapped to disk
        disable_mlock = true

        # listener denoting this is a worker proxy
        listener "tcp" {
          address = "0.0.0.0:9202"
          purpose = "proxy"
        }

        # worker block for configuring the specifics of the
        # worker service
        worker {
          name = "ingress-worker"
          public_addr = "$HOSTNAME"
          initial_upstreams = ["boundary-controller.default.svc.cluster.local:9201"]
          tags {
            type = ["worker1", "upstream"]
          }
        }

        # Events (logging) configuration. This
        # configures logging for ALL events to both
        # stderr and a file at /var/log/boundary/<boundary_use>.log
        events {
          audit_enabled       = true
          sysevents_enabled   = true
          observations_enable = true
          sink "stderr" {
            name = "all-events"
            description = "All events sent to stderr"
            event_types = ["*"]
            format = "cloudevents-json"
          }
          sink {
            name = "file-sink"
            description = "All events sent to a file"
            event_types = ["*"]
            format = "cloudevents-json"
            file {
              path = "/var/log/boundary"
              file_name = "ingress-worker.log"
            }
            audit_config {
              audit_filter_overrides {
                sensitive = "redact"
                secret    = "redact"
              }
            }
          }
        }

        # kms block for encrypting the authentication material
        kms "aead" {
          purpose = "worker-auth"
          aead_type = "aes-gcm"
          key = "8fZBjCUfN0TzjEGLQldGY4+iE9AkOvCfjh7+p0GtRBQ="
          key_id = "global_worker-auth"
        }
      service:
        type: ClusterIP
        name: ingress-worker-svc
        ports:
        - name: sesssion-proxy
          port: 9202


postgres:
  name: postgres-boundary
  port: 5432
  pvc:
    # init with "true" to test db migration with helm update of config
    enabled: false
    storage: 200M
  replicas: 1
  # postgres env vars required for bootstrap
  env:
    # POSTGRES_DB
    postgresDB: boundary
    # POSTGRES_USER
    postgresUsr: postgres
    # POSTGRES_PASSWORD
    postgresPswd: postgres
  
  readinessProbe:
    exec:
      command:
        - pg_isready
        - --host=127.0.0.1
        - --port=5432
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 3

  livenessProbe:
    exec:
      command: ["psql", "-w", "-U", "postgres", "-d", "boundary", "-c", "SELECT 1"]

# vault subchart included with override values for dev mode and injector; see vault helm repo for complete values.yaml
vault:
  global:
    enabled: false
  server:
    enabled: false
    enterpriseLicense:
      # The name of the Kubernetes secret that holds the enterprise license. The
      # secret must be in the same namespace that Vault is installed into.
      secretName: ""
      # The key within the Kubernetes secret that holds the enterprise license.
      secretKey: "license"
    image:
      # change to "hashicorp/vault" if no license supplied
      repository: "hashicorp/vault-enterprise"
      tag: "latest"
      # Overrides the default Image Pull Policy
      pullPolicy: IfNotPresent
    dev:
      enabled: true
      devRootToken: "root"
  injector:
    enabled: false