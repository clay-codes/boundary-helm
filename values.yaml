# Defaults to bare-bones boundary installation without auth method, host resources, scopes, targets, or initial login role creation
# requires recovery key workflow to run commands
# see here for more info: https://developer.hashicorp.com/boundary/docs/install-boundary/no-gen-resources
# if quick POC is desired instead, set dev to "true" below
boundary: 
  replicas: 1
    # TO ENABLE ENTERPRISE
    # set licenseSecretName to k8s secret for the boundary enterprise license 
    # template will auto update repo to enterprise, and auto append -ent to tag, regardless if tag already has -ent suffix
    # example secret creation:
    # kubectl create secret generic boundary-license --from-literal=license=${BOUNDARY_LICENSE}
    # with 'boundary-license' being the value for licenseSecretName etc
  image:
    licenseSecretName: "boundary-license"
    licenseSecretKey: license
    repository: hashicorp/boundary
    tag: latest
    pullPolicy: IfNotPresent
  
  setup:
    # dev mode creates root user "admin" with password "password"; run "boundary authenticate" to login
    # defaults to true for quick poc; set to false to allow for below 
    dev: false
    
    # migrateDB is used only after initialization if you wish to update the config
    # postgres pvc must have been created upon initialization (line 163)
    # if updating "root" kms key in config, KMS migration is also required: 
    # - keep the old "root" kms stanza in config along with your desired new "root" kms stanza
    # - change the 'purpose' parameter of this old kms stanza from "root" to "previous-root"
    migrateDB: false

    # setting to false as will require recovery key (kms) workflow and bootsrap w/o any resources
    # kms workflow outlined here https://developer.hashicorp.com/boundary/docs/install-boundary/no-gen-resources 
    # set to true to start in non-dev with all init options from here: 
    # https://developer.hashicorp.com/boundary/docs/commands/database/init#init-options 
    # must tail logs of init container to get credentials: 
    # kubectl logs boundary-controller-0 boundary-init -f
    withResources: true

  
  config: |
    disable_mlock = true

    controller {
      name = "kubernetes-controller"
      description = "A controller for a kubernetes demo!"
      database {
        url = "env://BOUNDARY_PG_URL"
      }
      public_cluster_addr = "boundary-controller:9201"
    }


    listener "tcp" {
      address = "0.0.0.0:9200"
      purpose = "api"
      tls_disable = true
    }

    listener "tcp" {
      address = "0.0.0.0:9201"
      purpose = "cluster"
      tls_disable = true
    }

    listener "tcp" {
      address = "0.0.0.0:9203"
      purpose = "proxy"
      tls_disable = true
    }

    kms "aead" {
      purpose = "root"
      aead_type = "aes-gcm"
      key = "sP1fnF5Xz85RrXyELHFeZg9Ad2qt4Z4bgNHVGtD6ung="
      key_id = "global_root"
    }
    
    kms "aead" {
      purpose = "worker-auth"
      aead_type = "aes-gcm"
      key = "8fZBjCUfN0TzjEGLQldGY4+iE9AkOvCfjh7+p0GtRBQ="
      key_id = "global_worker-auth"
    }

    kms "aead" {
      purpose = "recovery"
      aead_type = "aes-gcm"
      key = "8fZBjCUfN0TzjEGLQldGY4+iE9AkOvCfjh7+p0GtRBQ="
      key_id = "global_recovery"
    }

    # needed for session recording
    kms "aead" {
      purpose = "bsr"
      aead_type = "aes-gcm"
      key = "8fZBjCUfN0TzjEGLQldGY4+iE9AkOvCfjh7+p0GtRBQ="
      key_id = "global_bsr"
    }

  readinessProbe:
    exec:
      command:
        - sh
        - -c
        - "nc -zv postgres-boundary 5432"
    initialDelaySeconds: 11
    periodSeconds: 3
    timeoutSeconds: 3
    failureThreshold: 3

  livenessProbe: 
    httpGet:
      path: "/"
      port: 9200

  serviceAccount:
    # Specifies whether a service account should be created
    create: false
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  podAnnotations: {}

  service:
    type: ClusterIP
    ports:
    - name: api
      port: 9200
    - name: cluster
      port: 9201
    - name: data
      port: 9203
    - name: client
      port: 443

  ingress:
    enabled: false
    className: ""
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  resources:
    # for minikube mac m1
    requests:
      cpu: "100m"  # 0.1 CPU cores
      memory: "256Mi"  # 256 MiB of memory
    limits:
      cpu: "500m"  # 0.5 CPU cores
      memory: "1Gi"  # 1 GiB of memory  
  
  worker:
    # quick way to disable all workers and keep controller running
    disableAll: true
    ingress:
      enabled: true 
      replicas: 1
      # adapted from: 
      # https://developer.hashicorp.com/boundary/tutorials/enterprise/ent-deployment-guide#configure-boundary-workers
      config: |
        # disable memory from being swapped to disk
        disable_mlock = true

        # listener denoting this is a worker due to "proxy" purpose on 9202
        listener "tcp" {
          address = "0.0.0.0:9202"
          purpose = "proxy"
        }

        worker {
          name = "ingress-worker"
          initial_upstreams = ["boundary-controller:9201"]
          public_addr = "ingress-worker:9202"
          tags {
            type = ["worker1", "upstream"]
          }
        }

        # Events (logging) configuration. This
        # configures logging for ALL events to both
        # stderr and a file at /var/log/boundary/<boundary_use>.log
        events {
          audit_enabled       = true
          sysevents_enabled   = true
          observations_enable = true
          sink "stderr" {
            name = "all-events"
            description = "All events sent to stderr"
            event_types = ["*"]
            format = "cloudevents-json"
          }
          sink {
            name = "file-sink"
            description = "All events sent to a file"
            event_types = ["*"]
            format = "cloudevents-json"
            file {
              path = "/var/log/boundary"
              file_name = "ingress-worker.log"
            }
            audit_config {
              audit_filter_overrides {
                sensitive = "redact"
                secret    = "redact"
              }
            }
          }
        }

        # kms authentication circuvents need to manually register workers with controller
        # must match controller's worker-auth block exactly
        kms "aead" {
          purpose = "worker-auth"
          aead_type = "aes-gcm"
          key = "8fZBjCUfN0TzjEGLQldGY4+iE9AkOvCfjh7+p0GtRBQ="
          key_id = "global_worker-auth"
        }

        # uncomment to encrypt worker-auth keys in storage:
        # https://developer.hashicorp.com/boundary/tutorials/enterprise/ent-deployment-guide#prepare-worker-kms-keys
        # kms "aead" {
        #   purpose = "worker-auth-storage"
        #   aead_type = "aes-gcm"
        #   key = "8fZBjCUfN0TzjEGLQldGY4+iE9AkOvCfjh7+p0GtRBQ="
        #   key_id = "global_worker-auth"
        # }

        # simple aead key to auth downstream workers with KMS auth flow:
        # https://developer.hashicorp.com/boundary/docs/configuration/worker/worker-configuration#kms-led-authorization-authentication-flow
        # corresponding block in workers must have the 'worker-auth' purpose, with all other values matching exactly
        # generate aead key via 'openssl rand -base64 32'
        kms "aead" {
          purpose = "downstream-worker-auth"
          aead_type = "aes-gcm"
          key = "vAdHal5kv6LyEjEOcIhuVtEqYh78jwPgvizOUT+cSxs="
          key_id = "intermediate-auth"
        }

        # if no intermediate desired, comment out above kms block and uncomment below
        # this key can be used for direct egress worker auth to ingress
        # also requires initial_upstreams to be uncommented in worker block of egress config below
        # kms "aead" {
        #   purpose = "downstream-worker-auth"
        #   aead_type = "aes-gcm"
        #   key = "pAR+CY9FI0+77ey12GQ0ShyUp8adGvUvEQn3V9Sw6mg="
        #   key_id = "egress-auth"
        # }

      service:
        type: ClusterIP
        ports:
        - name: sesssion-proxy
          port: 9202
    
    intermediate:
      enabled: true
      replicas: 1
      config: |
        disable_mlock = true

        listener "tcp" {
          address = "0.0.0.0:9202"
          purpose = "proxy"
        }

        worker {
          name = "intermediate-worker"
          public_addr = "intermediate-worker:9202"
          initial_upstreams = ["ingress-worker:9202"]
          tags {
            type = ["worker2", "intermediate"]
          }
        }

        events {
          audit_enabled       = true
          sysevents_enabled   = true
          observations_enable = true
          sink "stderr" {
            name = "all-events"
            description = "All events sent to stderr"
            event_types = ["*"]
            format = "cloudevents-json"
          }
          sink {
            name = "file-sink"
            description = "All events sent to a file"
            event_types = ["*"]
            format = "cloudevents-json"
            file {
              path = "/var/log/boundary"
              file_name = "intermediate-worker.log"
            }
            audit_config {
              audit_filter_overrides {
                sensitive = "redact"
                secret    = "redact"
              }
            }
          }
        }
        
        # needed to auth upstream
        # all values except purpose must be identical else logs will show tls handshake error
        kms "aead" {
          purpose = "worker-auth"
          aead_type = "aes-gcm"
          key = "vAdHal5kv6LyEjEOcIhuVtEqYh78jwPgvizOUT+cSxs="
          key_id = "intermediate-auth"
        }

        # uncomment to encrypt worker auth keys in storage
        # kms "aead" {
        #   purpose = "worker-auth-storage"
        #   aead_type = "aes-gcm"
        #   key = "vAdHal5kv6LyEjEOcIhuVtEqYh78jwPgvizOUT+cSxs="
        #   key_id = "intermediate-auth"
        # }

        # needed to auth downstream worker
        kms "aead" {
          purpose = "downstream-worker-auth"
          aead_type = "aes-gcm"
          key = "pAR+CY9FI0+77ey12GQ0ShyUp8adGvUvEQn3V9Sw6mg="
          key_id = "egress-auth"
        }

      service:
        type: ClusterIP
        ports:
        - name: sesssion-proxy
          port: 9202

    egress:
      enabled: true
      replicas: 1
      config: |
        disable_mlock = true

        listener "tcp" {
          address = "0.0.0.0:9202"
          purpose = "proxy"
        }

        # uncomment/comment-out initial upstreams if opting for no intermediate worker
        worker {
          name = "egress-worker"
          public_addr = "egress-worker:9202"
          initial_upstreams = ["intermediate-worker:9202"]
          # initial upstreams = ["ingress-worker:9202"]
          tags {
            type = ["worker3", "egress"]
          }
        }

        events {
          audit_enabled       = true
          sysevents_enabled   = true
          observations_enable = true
          sink "stderr" {
            name = "all-events"
            description = "All events sent to stderr"
            event_types = ["*"]
            format = "cloudevents-json"
          }
          sink {
            name = "file-sink"
            description = "All events sent to a file"
            event_types = ["*"]
            format = "cloudevents-json"
            file {
              path = "/var/log/boundary"
              file_name = "egress-worker.log"
            }
            audit_config {
              audit_filter_overrides {
                sensitive = "redact"
                secret    = "redact"
              }
            }
          }
        }

        kms "aead" {
          purpose = "worker-auth"
          aead_type = "aes-gcm"
          key = "pAR+CY9FI0+77ey12GQ0ShyUp8adGvUvEQn3V9Sw6mg="
          key_id = "egress-auth"
        }
      
        # note the key must match the worker-auth key
        # kms "aead" {
        #   purpose = "worker-auth-storage"
        #   aead_type = "aes-gcm"
        #   key = "pAR+CY9FI0+77ey12GQ0ShyUp8adGvUvEQn3V9Sw6mg="
        #   key_id = "egress-auth"
        # }
      service:
        type: ClusterIP
        ports:
        - name: sesssion-proxy
          port: 9202

target:
  # alpine sts with ssh and psql for inspecting controller postgres db
  ssh:
    enabled: true
    ports:
    - name: ssh
      port: 22
    - name: client
      port: 443

postgres:
  pvc:
    # init with "true" to test db migration with helm update of config
    enabled: false
    storage: 200M
  replicas: 1
  # postgres env vars required for bootstrap
  env:
    # POSTGRES_DB
    postgresDB: boundary
    # POSTGRES_USER
    postgresUsr: postgres
    # POSTGRES_PASSWORD
    postgresPswd: postgres
  
  readinessProbe:
    exec:
      command:
        - pg_isready
        - --host=127.0.0.1
        - --port=5432
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 3

  livenessProbe:
    exec:
      command: ["psql", "-w", "-U", "postgres", "-d", "boundary", "-c", "SELECT 1"]

# vault subchart included with override values for dev mode and injector; see vault helm repo for complete values.yaml
vault:
  global:
    enabled: false
  server:
    enabled: false
    enterpriseLicense:
      # The name of the Kubernetes secret that holds the enterprise license. The
      # secret must be in the same namespace that Vault is installed into.
      secretName: ""
      # The key within the Kubernetes secret that holds the enterprise license.
      secretKey: "license"
    image:
      # change to "hashicorp/vault" if no license supplied
      repository: "hashicorp/vault-enterprise"
      tag: "latest"
      # Overrides the default Image Pull Policy
      pullPolicy: IfNotPresent
    dev:
      enabled: true
      devRootToken: "root"
  injector:
    enabled: false